{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from collections import defaultdict\n",
    "#from plot_attention import plot_attention\n",
    "import random\n",
    "import math\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "import dynet as dy\n",
    "import numpy as np\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_src_file = \"../data/parallel/train.ja\"\n",
    "train_trg_file = \"../data/parallel/train.en\"\n",
    "dev_src_file = \"../data/parallel/dev.ja\"\n",
    "dev_trg_file = \"../data/parallel/dev.en\"\n",
    "test_src_file = \"../data/parallel/test.ja\"\n",
    "test_trg_file = \"../data/parallel/test.en\"\n",
    "\n",
    "w2i_src = defaultdict(lambda: len(w2i_src))\n",
    "w2i_trg = defaultdict(lambda: len(w2i_trg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(sorted_dataset, max_batch_size):\n",
    "    source = [x[0] for x in sorted_dataset]\n",
    "    src_lengths = [len(x) for x in source]\n",
    "    batches = []\n",
    "    prev = src_lengths[0]\n",
    "    prev_start = 0\n",
    "    batch_size = 1\n",
    "    for i in range(1, len(src_lengths)):\n",
    "        if src_lengths[i] != prev or batch_size == max_batch_size:\n",
    "            batches.append((prev_start, batch_size))\n",
    "            prev = src_lengths[i]\n",
    "            prev_start = i\n",
    "            batch_size = 1\n",
    "        else:\n",
    "            batch_size += 1\n",
    "    return batches\n",
    "\n",
    "def read(fname_src, fname_trg):\n",
    "    \"\"\"\n",
    "    Read parallel files where each line lines up\n",
    "    \"\"\"\n",
    "    with open(fname_src, \"r\") as f_src, open(fname_trg, \"r\") as f_trg:\n",
    "        for line_src, line_trg in zip(f_src, f_trg):\n",
    "            #need to append EOS tags to at least the target sentence\n",
    "            sent_src = [w2i_src[x] for x in line_src.strip().split() + ['</s>']] \n",
    "            sent_trg = [w2i_trg[x] for x in ['<s>'] + line_trg.strip().split() + ['</s>']] \n",
    "            yield (sent_src, sent_trg)\n",
    "\n",
    "train = list(read(train_src_file, train_trg_file))\n",
    "unk_src = w2i_src[\"<unk>\"]\n",
    "eos_src = w2i_src['</s>']\n",
    "w2i_src = defaultdict(lambda: unk_src, w2i_src)\n",
    "unk_trg = w2i_trg[\"<unk>\"]\n",
    "eos_trg = w2i_trg['</s>']\n",
    "sos_trg = w2i_trg['<s>']\n",
    "w2i_trg = defaultdict(lambda: unk_trg, w2i_trg)\n",
    "i2w_trg = {v: k for k, v in w2i_trg.items()}\n",
    "i2w_src = {v: k for k, v in w2i_src.items()}\n",
    "\n",
    "nwords_src = len(w2i_src)\n",
    "nwords_trg = len(w2i_trg)\n",
    "dev = list(read(dev_src_file, dev_trg_file))\n",
    "test = list(read(test_src_file, test_trg_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DyNet Starts\n",
    "model = dy.Model()\n",
    "trainer = dy.AdamTrainer(model)\n",
    "\n",
    "# Model parameters\n",
    "EMBED_SIZE = 64\n",
    "HIDDEN_SIZE = 128\n",
    "ATTENTION_SIZE = 128\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "#Especially in early training, the model can generate basically infinitly without generating an EOS\n",
    "#have a max sent size that you end at\n",
    "MAX_SENT_SIZE = 50\n",
    "\n",
    "# Lookup parameters for word embeddings\n",
    "LOOKUP_SRC = model.add_lookup_parameters((nwords_src, EMBED_SIZE))\n",
    "LOOKUP_TRG = model.add_lookup_parameters((nwords_trg, EMBED_SIZE))\n",
    "\n",
    "# Word-level LSTMs\n",
    "\n",
    "#we want a bidirectional LSTM for attention over the source\n",
    "LSTM_SRC = dy.BiRNNBuilder(1, EMBED_SIZE, HIDDEN_SIZE, model, dy.LSTMBuilder)\n",
    "LSTM_TRG_BUILDER = dy.LSTMBuilder(1, EMBED_SIZE, HIDDEN_SIZE, model, dy.LSTMBuilder)\n",
    "\n",
    "#hidden size*3 to hidden size-layer before softmax in attention\n",
    "W_m_p = model.add_parameters((HIDDEN_SIZE, HIDDEN_SIZE*2))\n",
    "b_m_p = model.add_parameters(HIDDEN_SIZE)\n",
    "\n",
    "#the softmax from the hidden size \n",
    "W_sm_p = model.add_parameters((nwords_trg, HIDDEN_SIZE))         # Weights of the softmax\n",
    "b_sm_p = model.add_parameters((nwords_trg))                   # Softmax bias\n",
    "\n",
    "#parameters for attention\n",
    "w1_att_src_p = model.add_parameters((ATTENTION_SIZE, HIDDEN_SIZE))\n",
    "w1_att_tgt_p = model.add_parameters((ATTENTION_SIZE, HIDDEN_SIZE))\n",
    "w2_att_p = model.add_parameters((ATTENTION_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_attention(src_output_matrix, tgt_output_embedding, fixed_attentional_component):\n",
    "    w1_att_src = dy.parameter(w1_att_src_p)\n",
    "    w1_att_tgt = dy.parameter(w1_att_tgt_p)\n",
    "    w2_att = dy.parameter(w2_att_p)\n",
    "    a_t = dy.transpose(dy.tanh(dy.colwise_add(fixed_attentional_component, w1_att_tgt * tgt_output_embedding))) * w2_att\n",
    "    alignment = dy.softmax(a_t)\n",
    "    att_output = src_output_matrix * alignment\n",
    "    return att_output, alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
